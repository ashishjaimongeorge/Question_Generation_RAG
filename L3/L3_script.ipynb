{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS3Zv-azg9DB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from datetime import datetime\n",
        "import openai\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import json\n",
        "\n",
        "# Load sensitive keys from environment variables\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Initialize Pinecone client\n",
        "pc = Pinecone(\n",
        "    api_key=PINECONE_API_KEY,\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\"\n",
        "    )\n",
        ")\n",
        "index_name = \"project-management-rag\"\n",
        "\n",
        "# Create Pinecone index if it doesn't exist\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud=\"aws\",\n",
        "            region=\"us-east-1\"\n",
        "        )\n",
        "    )\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Set OpenAI API Key\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Fine-tuned model ID\n",
        "FINE_TUNED_MODEL = \"ft:gpt-4o-mini-2024-07-18:ct-main-test::AZBbHLRk\"\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file, page by page.\"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        return [{\"page_number\": page_number + 1, \"text\": page.extract_text()} for page_number, page in enumerate(reader.pages)]\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to extract text from PDF: {e}\")\n",
        "\n",
        "def generate_embedding(text):\n",
        "    \"\"\"Generate embeddings for the given text using OpenAI's API.\"\"\"\n",
        "    try:\n",
        "        response = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n",
        "        return response['data'][0]['embedding']\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to generate embedding: {e}\")\n",
        "\n",
        "def index_text_segments(text_segments):\n",
        "    \"\"\"Index text segments into Pinecone.\"\"\"\n",
        "    try:\n",
        "        for segment in text_segments:\n",
        "            embedding = generate_embedding(segment['text'])\n",
        "            index.upsert([\n",
        "                (\n",
        "                    f\"page-{segment['page_number']}\",\n",
        "                    embedding,\n",
        "                    {\"text\": segment['text'], \"page_number\": segment['page_number']}\n",
        "                )\n",
        "            ])\n",
        "        print(\"Text segments indexed successfully!\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to index text segments: {e}\")\n",
        "\n",
        "def extract_main_topics(text_segments):\n",
        "    \"\"\"Extract main topics from text segments using a fine-tuned OpenAI model.\"\"\"\n",
        "    try:\n",
        "        joined_text = \" \".join([segment['text'] for segment in text_segments[:5]])\n",
        "        prompt = f\"Extract the main topics from the following text:\\n{joined_text}\\n\\nProvide the topics as a numbered list.\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=FINE_TUNED_MODEL,\n",
        "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                      {\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=500\n",
        "        )\n",
        "        topics = response.choices[0].message['content'].split(\"\\n\")\n",
        "        return [topic.strip() for topic in topics if topic.strip()]\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to extract main topics: {e}\")\n",
        "\n",
        "def retrieve_context_for_topic(topic):\n",
        "    \"\"\"Retrieve context for a given topic using Pinecone.\"\"\"\n",
        "    try:\n",
        "        topic_embedding = generate_embedding(topic)\n",
        "        results = index.query(vector=topic_embedding, top_k=5, include_metadata=True)\n",
        "        context_chunks = []\n",
        "        for match in results['matches']:\n",
        "            context_chunks.append({\n",
        "                \"text\": match['metadata']['text'],\n",
        "                \"page_number\": match['metadata']['page_number'],\n",
        "                \"confidence_score\": match['score']\n",
        "            })\n",
        "        return context_chunks\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to retrieve context for topic: {e}\")\n",
        "\n",
        "def generate_questions_for_topic(topic, context_chunks):\n",
        "    \"\"\"Generate questions for a topic using the context chunks.\"\"\"\n",
        "    try:\n",
        "        best_chunk = max(context_chunks, key=lambda x: x[\"confidence_score\"])\n",
        "        context_text = best_chunk[\"text\"]\n",
        "        source_page = best_chunk[\"page_number\"]\n",
        "        confidence_score = best_chunk[\"confidence_score\"]\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Based on the following topic: \"{topic}\" and the context: \"{context_text}\",\n",
        "        generate 5 multiple-choice questions with:\n",
        "        - Four options labeled A, B, C, and D\n",
        "        - The correct answer\n",
        "        - Detailed explanations for the correct answers\n",
        "\n",
        "        Provide the output in this format:\n",
        "        {{\n",
        "            \"questions\": [\n",
        "                {{\n",
        "                    \"question\": \"<Question text>\",\n",
        "                    \"options\": [\"A) <Option 1>\", \"B) <Option 2>\", \"C) <Option 3>\", \"D) <Option 4>\"],\n",
        "                    \"correct_answer\": \"<Correct Option Label>\",\n",
        "                    \"explanation\": \"<Detailed explanation>\"\n",
        "                }}\n",
        "            ]\n",
        "        }}\n",
        "        \"\"\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=FINE_TUNED_MODEL,\n",
        "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                      {\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=1500\n",
        "        )\n",
        "        questions = json.loads(response.choices[0].message['content'])[\"questions\"]\n",
        "\n",
        "        for question in questions:\n",
        "            question[\"source_page\"] = source_page\n",
        "            question[\"confidence_score\"] = confidence_score\n",
        "\n",
        "        return questions\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to generate questions for topic: {e}\")\n",
        "\n",
        "def format_output(topics, questions):\n",
        "    \"\"\"Format the output data for topics and questions.\"\"\"\n",
        "    topics_data = {\n",
        "        \"book_title\": \"Project Management Professional Guide\",\n",
        "        \"total_topics\": len(topics),\n",
        "        \"extraction_timestamp\": datetime.now().isoformat(),\n",
        "        \"main_topics\": topics\n",
        "    }\n",
        "\n",
        "    questions_data = {\n",
        "        \"metadata\": {\n",
        "            \"generated_at\": datetime.now().isoformat(),\n",
        "            \"total_questions\": sum(len(q['questions']) for q in questions),\n",
        "            \"book_title\": \"Project Management Professional Guide\",\n",
        "            \"generation_method\": \"RAG Pipeline\",\n",
        "            \"embedding_model\": \"text-embedding-ada-002\",\n",
        "            \"vector_store\": \"Pinecone\"\n",
        "        },\n",
        "        \"questions\": questions\n",
        "    }\n",
        "\n",
        "    return topics_data, questions_data\n",
        "\n",
        "def save_to_json(data, filename):\n",
        "    \"\"\"Save data to a JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(filename, \"w\") as f:\n",
        "            json.dump(data, f, indent=4)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to save data to JSON file: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        pdf_path = \"/content/Project.pdf\"\n",
        "        text_segments = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        index_text_segments(text_segments)\n",
        "\n",
        "        main_topics = extract_main_topics(text_segments)\n",
        "\n",
        "        questions = []\n",
        "        for topic in main_topics:\n",
        "            context_chunks = retrieve_context_for_topic(topic)\n",
        "            question_data = generate_questions_for_topic(topic, context_chunks)\n",
        "            questions.append({\"topic\": topic, \"questions\": question_data})\n",
        "\n",
        "        topics_data, questions_data = format_output(main_topics, questions)\n",
        "        save_to_json(topics_data, \"topics.json\")\n",
        "        save_to_json(questions_data, \"questions.json\")\n",
        "\n",
        "        print(\"RAG-based question generation completed and saved to JSON!\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ]
    }
  ]
}