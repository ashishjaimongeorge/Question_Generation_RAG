{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pN8bM1sahQZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from datetime import datetime\n",
        "import openai\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Set API keys from environment variables for security\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(\n",
        "    api_key=PINECONE_API_KEY,\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\"\n",
        "    )\n",
        ")\n",
        "index_name = \"project-management-rag\"\n",
        "\n",
        "# Create index if it doesn't exist\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud=\"aws\",\n",
        "            region=\"us-east-1\"\n",
        "        )\n",
        "    )\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Set OpenAI API key\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    return [{\"page_number\": page_number + 1, \"text\": page.extract_text()} for page_number, page in enumerate(reader.pages)]\n",
        "\n",
        "def generate_embedding(text):\n",
        "    response = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n",
        "    return response['data'][0]['embedding']\n",
        "\n",
        "def index_text_segments(text_segments):\n",
        "    for segment in text_segments:\n",
        "        embedding = generate_embedding(segment['text'])\n",
        "        index.upsert([\n",
        "            (\n",
        "                f\"page-{segment['page_number']}\",\n",
        "                embedding,\n",
        "                {\"text\": segment['text'], \"page_number\": segment['page_number']}\n",
        "            )\n",
        "        ])\n",
        "    print(\"Text segments indexed successfully!\")\n",
        "\n",
        "def extract_main_topics(text_segments):\n",
        "    joined_text = \" \".join([segment['text'] for segment in text_segments[:5]])\n",
        "    prompt = f\"Extract the main topics from the following text:\\n{joined_text}\\n\\nProvide the topics as a numbered list.\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4-0613\",\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                  {\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=500\n",
        "    )\n",
        "    topics = response.choices[0].message['content'].split(\"\\n\")\n",
        "    return [topic.strip() for topic in topics if topic.strip()]\n",
        "\n",
        "def retrieve_context_for_topic(topic):\n",
        "    topic_embedding = generate_embedding(topic)\n",
        "    results = index.query(vector=topic_embedding, top_k=5, include_metadata=True)\n",
        "    context_chunks = []\n",
        "    for match in results['matches']:\n",
        "        context_chunks.append({\n",
        "            \"text\": match['metadata']['text'],\n",
        "            \"page_number\": match['metadata']['page_number'],\n",
        "            \"confidence_score\": match['score']\n",
        "        })\n",
        "    return context_chunks\n",
        "\n",
        "def generate_questions_for_topic(topic, context_chunks):\n",
        "    best_chunk = max(context_chunks, key=lambda x: x[\"confidence_score\"])\n",
        "    context_text = best_chunk[\"text\"]\n",
        "    source_page = best_chunk[\"page_number\"]\n",
        "    confidence_score = best_chunk[\"confidence_score\"]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Based on the following topic: \"{topic}\" and the context: \"{context_text}\",\n",
        "    generate 5 multiple-choice questions with:\n",
        "    - Four options labeled A, B, C, and D\n",
        "    - The correct answer\n",
        "    - Detailed explanations for the correct answers\n",
        "\n",
        "    Provide the output strictly in this JSON format:\n",
        "    {{\n",
        "        \"questions\": [\n",
        "            {{\n",
        "                \"question\": \"What is a project?\",\n",
        "                \"options\": [\"A) Definition 1\", \"B) Definition 2\", \"C) Definition 3\", \"D) Definition 4\"],\n",
        "                \"correct_answer\": \"A\",\n",
        "                \"explanation\": \"Explanation for the correct answer.\"\n",
        "            }}\n",
        "        ]\n",
        "    }}\n",
        "    \"\"\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4-0613\",\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                  {\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=1500\n",
        "    )\n",
        "    raw_content = response.choices[0].message['content']\n",
        "\n",
        "    # Try parsing the JSON response\n",
        "    try:\n",
        "        questions = json.loads(raw_content)[\"questions\"]\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON parsing error: {e}\")\n",
        "        print(\"Raw response:\", raw_content)\n",
        "        cleaned_content = re.sub(r'(?<!\\\\)\"(?![:,}\\]])', r'\\\"', raw_content)\n",
        "        questions = json.loads(cleaned_content)[\"questions\"]\n",
        "\n",
        "    for question in questions:\n",
        "        question[\"source_page\"] = source_page\n",
        "        question[\"confidence_score\"] = confidence_score\n",
        "\n",
        "    return questions\n",
        "\n",
        "def format_output(topics, questions):\n",
        "    topics_data = {\n",
        "        \"book_title\": \"Project Management Professional Guide\",\n",
        "        \"total_topics\": len(topics),\n",
        "        \"extraction_timestamp\": datetime.now().isoformat(),\n",
        "        \"main_topics\": topics\n",
        "    }\n",
        "\n",
        "    questions_data = {\n",
        "        \"metadata\": {\n",
        "            \"generated_at\": datetime.now().isoformat(),\n",
        "            \"total_questions\": sum(len(q['questions']) for q in questions),\n",
        "            \"book_title\": \"Project Management Professional Guide\",\n",
        "            \"generation_method\": \"RAG Pipeline\",\n",
        "            \"embedding_model\": \"text-embedding-ada-002\",\n",
        "            \"vector_store\": \"Pinecone\"\n",
        "        },\n",
        "        \"questions\": questions\n",
        "    }\n",
        "\n",
        "    return topics_data, questions_data\n",
        "\n",
        "def save_to_json(data, filename):\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_path = \"/content/Project.pdf\"\n",
        "    text_segments = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    index_text_segments(text_segments)\n",
        "\n",
        "    main_topics = extract_main_topics(text_segments)\n",
        "\n",
        "    questions = []\n",
        "    for topic in main_topics:\n",
        "        context_chunks = retrieve_context_for_topic(topic)\n",
        "        question_data = generate_questions_for_topic(topic, context_chunks)\n",
        "        questions.append({\"topic\": topic, \"questions\": question_data})\n",
        "\n",
        "    topics_data, questions_data = format_output(main_topics, questions)\n",
        "    save_to_json(topics_data, \"topics.json\")\n",
        "    save_to_json(questions_data, \"questions.json\")\n",
        "\n",
        "    print(\"RAG-based question generation completed and saved to JSON!\")\n"
      ]
    }
  ]
}